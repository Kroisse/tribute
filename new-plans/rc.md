# Reference Counting (Native Backend)

> This document defines the RC memory management strategy for the Cranelift
> native backend. The WASM backend uses WasmGC and is not affected.
>
> See also: [cranelift-backend.md](cranelift-backend.md),
> [implementation.md](implementation.md)

## Overview

The native backend uses **reference counting** for heap-allocated objects
(structs, enums, arrays, boxed primitives). Key principles:

- **No runtime library** — all RC logic is compiler-generated code
- **libc only** — depends solely on `malloc`/`free` (via allocator indirection)
- **Dialect-based** — RC operations are `tribute_rt.retain`/`tribute_rt.release`
  dialect ops, lowered to inline code
- **Phased rollout** — Phase 3a (shallow), 3b (deep release), 4 (continuations)

## Allocator Interface

All heap allocation goes through two indirection symbols:

```text
__tribute_alloc(size: i64) -> ptr
__tribute_dealloc(ptr: ptr, size: i64)
```

### Default Implementation

The compiler generates default implementations as simple `malloc`/`free`
wrappers. These are declared with `Import` linkage so they can be overridden
at link time (e.g., with a custom allocator via weak symbols).

### Alloc Sequence (compiler-generated inline)

```text
raw_ptr = call @__tribute_alloc(size + 8)    // include header
store refcount=1       at raw_ptr
store rtti_idx=<type>  at raw_ptr + 4
obj_ptr = raw_ptr + 8                        // caller sees offset 0
```

### Free Sequence (compiler-generated inline)

```text
raw_ptr = obj_ptr - 8
call @__tribute_dealloc(raw_ptr, size + 8)
```

### Symbol Convention

Internal/runtime symbols use the `__tribute_` prefix to avoid collisions
with user code and to clearly mark compiler-generated functions.

---

## Memory Layout

### Object Header (Phase 3a+)

Every heap-allocated RC object has an 8-byte header prepended before the
payload. Compiled code always sees the pointer at offset 0 (first field);
header access uses `ptr - 8`.

```text
[-8] refcount: u32   — reference count (1 on allocation)
[-4] rtti_idx: u32   — runtime type info index
[ 0] payload...      — first field (naturally aligned)
```

> **Current state (pre-RC):** Struct allocation via `adt_to_clif` and
> boxing via `tribute_rt_to_clif` do NOT include the header yet. The
> header will be added when the RC insertion pass (PR 3) lands.

### Struct Layout

Structs are laid out with fields in declaration order, naturally aligned:

```text
Struct: [fields in order, naturally aligned]
Enum:   [tag: i32] [padding] [payload: max(variant sizes)]
Array:  [length: i64] [elements...]
```

Field offsets are computed by `adt_layout.rs` at compile time.

### Boxed Primitives

Boxed primitives are the simplest heap objects — just the raw value:

| Type | Payload Size | Layout |
| ---- | ----------- | ------ |
| boxed i32 (Int/Nat/Bool) | 4 bytes | `[i32 value]` |
| boxed f64 (Float) | 8 bytes | `[f64 value]` |

---

## RC Operations

The `tribute_rt` dialect provides two RC operations:

```text
tribute_rt.retain(ptr) -> ptr    // refcount++, return same pointer
tribute_rt.release(ptr)          // refcount--, free if zero
```

These are dialect-level operations that will be:

1. **Inserted** by the RC insertion pass (SSA-based liveness analysis)
2. **Lowered** to inline code by the RC lowering pass

### Inline Lowering (Phase 3a)

```text
// tribute_rt.retain(ptr):
refcount = load(ptr - 8)
refcount = refcount + 1
store(refcount, ptr - 8)

// tribute_rt.release(ptr):
refcount = load(ptr - 8)
refcount = refcount - 1
store(refcount, ptr - 8)
if refcount == 0:
    call @__tribute_dealloc(ptr - 8, size + 8)
```

Phase 3b will replace the simple dealloc with type-specific destructors
via RTTI dispatch.

---

## Boxing / Unboxing

Boxing converts unboxed primitives to heap-allocated pointers for use in
polymorphic contexts (e.g., passing `Int` where `any` is expected).

### Implementation

Boxing and unboxing are handled at two levels:

1. **Explicit ops** (`tribute_rt.box_int`, `tribute_rt.unbox_int`, etc.)
   — generated by the `insert_boxing` pass, lowered by
   `tribute_rt_to_clif` to `clif.*` allocation + store/load.

2. **Implicit casts** (`unrealized_conversion_cast(i32 → ptr)`)
   — resolved by materializations in the native type converter.

Both paths generate equivalent code:

```text
// Boxing (e.g., i32 → ptr):
%size = clif.iconst(4)
%ptr  = clif.call @__tribute_alloc(%size)
clif.store(%value, %ptr, offset=0)
// result: %ptr

// Unboxing (e.g., ptr → i32):
%value = clif.load(%ptr, offset=0)
```

### Comparison with WASM Backend

| Aspect | WASM | Native |
| ------ | ---- | ------ |
| Int/Nat/Bool boxing | `wasm.ref_i31` (i31ref) | heap alloc + store |
| Float boxing | `wasm.struct_new` (BoxedF64) | heap alloc + store |
| Int/Nat unboxing | `wasm.i31_get_s/u` | `clif.load` |
| Float unboxing | `wasm.struct_get` | `clif.load` |
| Representation | GC-managed refs | raw pointers |

---

## RC Pipeline

The RC implementation is divided into three passes:

### 1. RC Insertion Pass (PR 3)

**Location:** `tribute-passes/src/native/rc_insertion.rs`

**Purpose:** Insert `tribute_rt.retain` and `tribute_rt.release` operations
based on SSA liveness analysis.

**Algorithm:**

1. **Liveness analysis** — determine last use of each SSA value
2. **Ownership rules:**
   - Function parameters: `retain` at entry block
   - Return values: ownership transfer (no retain/release)
   - Local allocations: refcount=1 at creation, `release` at last use
   - Struct field loads: `retain` after load, `release` at last use
   - Struct field stores: `release` old value before store
   - Branch targets: `retain` for each successor path

**Pointer detection:** Use `core::Ptr::from_type()` to identify RC-managed values.

### 2. RC Optimization Pass (Future)

**Location:** `tribute-passes/src/native/rc_optimization.rs`

**Purpose:** Eliminate redundant retain/release pairs.

**Optimizations:**

- **Paired elimination:** Remove adjacent `retain` followed by `release` on
  same value
- **Borrow analysis:** Identify temporary borrows that don't need RC ops
- **Constant propagation:** Elide RC for compile-time-known lifetimes

### 3. RC Lowering Pass (PR 4)

**Location:** `tribute-passes/src/native/rc_lowering.rs`

**Purpose:** Lower `tribute_rt.retain` and `tribute_rt.release` to inline
`clif.*` operations.

**Lowering patterns:**

```text
tribute_rt.retain(ptr) ->
    %rc_addr = clif.iadd(ptr, clif.iconst(-8))
    %rc = clif.load(%rc_addr)
    %new_rc = clif.iadd(%rc, clif.iconst(1))
    clif.store(%new_rc, %rc_addr)
    // result: ptr (unchanged)

tribute_rt.release(ptr) ->
    %rc_addr = clif.iadd(ptr, clif.iconst(-8))
    %rc = clif.load(%rc_addr)
    %new_rc = clif.isub(%rc, clif.iconst(1))
    clif.store(%new_rc, %rc_addr)
    %is_zero = clif.icmp(%new_rc, clif.iconst(0), cond="eq")
    clif.brif(%is_zero, then_dest=free_block, else_dest=continue_block)

free_block:
    %raw_ptr = clif.iadd(ptr, clif.iconst(-8))
    %size = clif.iconst(<object_size> + 8)
    clif.call(@__tribute_dealloc, %raw_ptr, %size)
    clif.jump(continue_block)

continue_block:
    // continue execution
```

**Pipeline position:** After `resolve_unrealized_casts`, before `emit_module_to_native`.

---

## Phasing

RC is implemented in three phases:

### Phase 3a: Shallow Free (Current PR + Next PR)

**Goal:** Basic RC infrastructure without deep release.

**Components:**

- ✅ **PR 1:** Allocator indirection (`__tribute_alloc`, `__tribute_dealloc`)
- ✅ **PR 2:** Boxing/unboxing + `retain`/`release` ops (this PR)
- ⏳ **PR 3:** RC insertion pass (SSA-based liveness)
- ⏳ **PR 4:** RC lowering pass (inline refcount ops)

**State after Phase 3a:**

- All heap objects have 8-byte headers (refcount + rtti_idx)
- Boxing/unboxing work for primitives
- `retain`/`release` inserted and lowered to inline code
- **Limitation:** `release` only does shallow free (no recursive release of fields)
- Leak detection: Use Valgrind/AddressSanitizer to verify no double-frees

### Phase 3b: Deep Release (Deferred)

**Goal:** Recursive release of pointer fields.

**New components:**

- **Type-specific release functions:** Compiler generates
  `__tribute_release_T(ptr)` for each struct type
- **RTTI table:** Maps `rtti_idx` to release function pointer
- **Deep release logic:** Release function calls `tribute_rt.release` on
  pointer fields before dealloc

**Example (struct with pointer field):**

```text
struct Point { x: Int, y: Ref<Node> }

// Compiler generates:
__tribute_release_Point(ptr):
    %y_addr = clif.iadd(ptr, clif.iconst(4))  // field offset
    %y_val = clif.load(%y_addr)
    tribute_rt.release(%y_val)                 // recursive release
    %raw = clif.iadd(ptr, clif.iconst(-8))
    call @__tribute_dealloc(%raw, clif.iconst(16))
```

**RTTI dispatch:**

```text
tribute_rt.release(ptr) ->
    %rc = decrement_refcount(ptr)
    if %rc == 0:
        %rtti_idx = load(ptr - 4)
        %release_fn = RTTI_TABLE[%rtti_idx].release_fn
        call %release_fn(ptr)
```

### Phase 4a: Continuation RC Protection (Implemented)

**Goal:** Prevent use-after-free when handlers release RC pointers that
live across `mp_yield` boundaries.

**Problem:** When `mp_yield` captures a continuation, the stack segment is
copied to the heap. If the handler releases a pointer that was live on the
captured stack, resuming the continuation accesses freed memory.

**Solution — three components:**

1. **`insert_rc` extension (Phase 2.8):** At each `__tribute_yield` call site,
   inserts extra `retain` for all live RC pointers before yield, and matching
   `release` after yield returns (resume path). Also stores the RC roots in
   TLS via `__tribute_yield_set_rc_roots`.

2. **`TributeContinuation` wrapper (runtime):** Opaque wrapper around the raw
   `mp_resume` pointer that carries an array of `RcObject` roots captured at
   yield time. Created by `__tribute_cont_wrap_from_tls`.

3. **`cont_rc` rewrite pass (Phase 2.85):** Rewrites handler dispatch code:
   - `__tribute_get_yield_continuation()` result → wrapped via
     `__tribute_cont_wrap_from_tls`
   - `__tribute_resume` → `__tribute_resume_safe`
   - `__tribute_resume_drop` → `__tribute_resume_drop_safe`

**RC flow:**

```text
Before yield:  retain(ptr)     → refcount + 1 (capture protection)
               store ptr in TLS RC roots
               __tribute_yield(...)

Resume path:   yield returns
               release(ptr)    → refcount - 1 (cancel extra retain)
               body continues with normal release path

Drop path:     __tribute_resume_drop_safe(wrapped_k)
               runtime: release each rc_root → refcount - 1
               runtime: mp_resume_drop → discard captured stack
```

**Known limitation:** On the drop path, the body's normal `release` calls
do not execute (the captured stack is discarded). This can cause leaks for
objects that would have been released by those calls. Phase 4b will address
this by generating cleanup functions for captured stack frames.

### Phase 4b: Continuation Cleanup Functions (Deferred)

**Goal:** Generate per-yield-site cleanup functions that release all RC
objects that the body would have released between yield and function exit.

**Approach:** At each yield site, statically determine which RC pointers
would be released on the normal execution path. Generate a cleanup function
that performs those releases, and attach it to the `TributeContinuation`
wrapper for invocation during `resume_drop`.

---

## RTTI Table

**Location (future):** Emitted as static data by `trunk-ir-cranelift-backend`

**Structure:**

```rust
struct TypeInfo {
    release_fn: extern "C" fn(*mut u8),  // Type-specific destructor
    size: u32,                            // Object size (excluding header)
    // Future: field_count, field_offsets, name, etc.
}

// Emitted as static data in each compiled module
static TRIBUTE_RTTI_TABLE: [TypeInfo; N] = [...];
```

**Index allocation:**

- Compile-time sequential assignment per module
- Reserved indices:
  - `0` = boxed i32 (Int)
  - `1` = boxed f64 (Float)
  - `2` = boxed i32 (Bool/Nat)
  - `3+` = user-defined structs/enums

**Phase 3a behavior:** `rtti_idx` is recorded but not used (all objects
use shallow free).

**Phase 3b behavior:** `release` dispatch via RTTI table for deep release.

---

## Field Reordering

**Goal:** Minimize struct padding by reordering fields by alignment.

**Rules:**

- Compiler MAY reorder struct fields for optimal layout
- Original field order preserved in `field_offsets` mapping
- All access via `adt.struct_get(field_idx)` uses offset from mapping

**Example:**

```text
// Source:
struct Foo { a: i8, b: i64, c: i16 }

// Reordered layout (8-byte alignment):
[b: i64] [c: i16] [a: i8] [padding: 5 bytes]  // total: 16 bytes

// vs. original order:
[a: i8] [padding: 7] [b: i64] [c: i16] [padding: 6]  // total: 24 bytes

// field_offsets mapping:
field_offsets[0] = 9   // a at byte 9
field_offsets[1] = 0   // b at byte 0
field_offsets[2] = 8   // c at byte 8
```

**Future:** Add `@repr(c)` attribute to disable reordering for FFI compatibility.

---

## Testing Strategy

### Unit Tests

- RC insertion: Verify retain/release placement via hand-crafted IR
- RC lowering: Verify inline code generation (refcount ops, conditional free)
- Boxing: Verify allocation + store sequences

### Integration Tests

- E2E: Tribute source → native binary with RC
- Memory safety: Valgrind/AddressSanitizer (no leaks, no double-frees)

### Test Scenarios

- **Pointer parameters:** Retain at entry, release at last use
- **Struct fields:** Release old value on field update
- **Polymorphic boxing:** Int → any → Int round-trip
- **Control flow:** Retain for multiple successors
- **Cyclic references:** (Future) Detect and handle cycles

---

## Deferred Decisions

- **Cycle detection:** Weak references? Tracing GC fallback?
- **Thread-safety:** Atomic refcount for multi-threaded code?
- **Continuation cleanup:** Phase 4b cleanup functions for drop-path leaks
- **FFI boundaries:** How to handle RC objects at C FFI boundaries?
- **Optimization:** Compile-time escape analysis to elide RC?
