# Reference Counting (Native Backend)

> This document defines the RC memory management strategy for the Cranelift
> native backend. The WASM backend uses WasmGC and is not affected.
>
> See also: [cranelift-backend.md](cranelift-backend.md),
> [implementation.md](implementation.md)

## Overview

The native backend uses **reference counting** for heap-allocated objects
(structs, enums, arrays, boxed primitives). Key principles:

- **No runtime library** — all RC logic is compiler-generated code
- **libc only** — depends solely on `malloc`/`free` (via allocator indirection)
- **Dialect-based** — RC operations are `tribute_rt.retain`/`tribute_rt.release`
  dialect ops, lowered to inline code
- **Phased rollout** — Phase 3a (shallow), 3b (deep release), 4 (continuations)

## Allocator Interface

All heap allocation goes through two indirection symbols:

```text
__tribute_alloc(size: i64) -> ptr
__tribute_dealloc(ptr: ptr, size: i64)
```

### Default Implementation

The compiler generates default implementations as simple `malloc`/`free`
wrappers. These are declared with `Import` linkage so they can be overridden
at link time (e.g., with a custom allocator via weak symbols).

### Alloc Sequence (compiler-generated inline)

```text
raw_ptr = call @__tribute_alloc(size + 8)    // include header
store refcount=1       at raw_ptr
store rtti_idx=<type>  at raw_ptr + 4
obj_ptr = raw_ptr + 8                        // caller sees offset 0
```

### Free Sequence (compiler-generated inline)

```text
raw_ptr = obj_ptr - 8
call @__tribute_dealloc(raw_ptr, size + 8)
```

### Symbol Convention

Internal/runtime symbols use the `__tribute_` prefix to avoid collisions
with user code and to clearly mark compiler-generated functions.

---

## Memory Layout

### Object Header (Phase 3a+)

Every heap-allocated RC object has an 8-byte header prepended before the
payload. Compiled code always sees the pointer at offset 0 (first field);
header access uses `ptr - 8`.

```text
[-8] refcount: u32   — reference count (1 on allocation)
[-4] rtti_idx: u32   — runtime type info index
[ 0] payload...      — first field (naturally aligned)
```

> **Current state (pre-RC):** Struct allocation via `adt_to_clif` and
> boxing via `tribute_rt_to_clif` do NOT include the header yet. The
> header will be added when the RC insertion pass (PR 3) lands.

### Struct Layout

Structs are laid out with fields in declaration order, naturally aligned:

```text
Struct: [fields in order, naturally aligned]
Enum:   [tag: i32] [padding] [payload: max(variant sizes)]
Array:  [length: i64] [elements...]
```

Field offsets are computed by `adt_layout.rs` at compile time.

### Boxed Primitives

Boxed primitives are the simplest heap objects — just the raw value:

| Type | Payload Size | Layout |
| ---- | ----------- | ------ |
| boxed i32 (Int/Nat/Bool) | 4 bytes | `[i32 value]` |
| boxed f64 (Float) | 8 bytes | `[f64 value]` |

---

## RC Operations

The `tribute_rt` dialect provides two RC operations:

```text
tribute_rt.retain(ptr) -> ptr    // refcount++, return same pointer
tribute_rt.release(ptr)          // refcount--, free if zero
```

These are dialect-level operations that will be:

1. **Inserted** by the RC insertion pass (SSA-based liveness analysis)
2. **Lowered** to inline code by the RC lowering pass

### Inline Lowering (Phase 3a)

```text
// tribute_rt.retain(ptr):
refcount = load(ptr - 8)
refcount = refcount + 1
store(refcount, ptr - 8)

// tribute_rt.release(ptr):
refcount = load(ptr - 8)
refcount = refcount - 1
store(refcount, ptr - 8)
if refcount == 0:
    call @__tribute_dealloc(ptr - 8, size + 8)
```

Phase 3b will replace the simple dealloc with type-specific destructors
via RTTI dispatch.

---

## Boxing / Unboxing

Boxing converts unboxed primitives to heap-allocated pointers for use in
polymorphic contexts (e.g., passing `Int` where `any` is expected).

### Implementation

Boxing and unboxing are handled at two levels:

1. **Explicit ops** (`tribute_rt.box_int`, `tribute_rt.unbox_int`, etc.)
   — generated by the `insert_boxing` pass, lowered by
   `tribute_rt_to_clif` to `clif.*` allocation + store/load.

2. **Implicit casts** (`unrealized_conversion_cast(i32 → ptr)`)
   — resolved by materializations in the native type converter.

Both paths generate equivalent code:

```text
// Boxing (e.g., i32 → ptr):
%size = clif.iconst(4)
%ptr  = clif.call @__tribute_alloc(%size)
clif.store(%value, %ptr, offset=0)
// result: %ptr

// Unboxing (e.g., ptr → i32):
%value = clif.load(%ptr, offset=0)
```

### Comparison with WASM Backend

| Aspect | WASM | Native |
| ------ | ---- | ------ |
| Int/Nat/Bool boxing | `wasm.ref_i31` (i31ref) | heap alloc + store |
| Float boxing | `wasm.struct_new` (BoxedF64) | heap alloc + store |
| Int/Nat unboxing | `wasm.i31_get_s/u` | `clif.load` |
| Float unboxing | `wasm.struct_get` | `clif.load` |
| Representation | GC-managed refs | raw pointers |

---

*Sections on RC Pipeline and Phasing will be added in subsequent PRs.*
